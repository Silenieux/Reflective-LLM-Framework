

import sqlite3
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import os
import requests
from datetime import datetime
from core.utils.reflection_safety_patch import soft_self_reflection_filter
from ext_modules import prompt_clinic

# Initialize database
conn = sqlite3.connect("memory/reflective_memory.db")
cursor = conn.cursor()

# Ensure the reflective_memory table exists
cursor.execute("""
CREATE TABLE IF NOT EXISTS reflective_memory (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    summary TEXT NOT NULL,
    reflective_prompt TEXT NOT NULL,
    tone_before TEXT,
    tone_after TEXT
)
""")
conn.commit()

from core.utils.llm_query import query_llm
from core.faiss_core.vector_search import get_similar_memories
from core.utils.web_tools import web_search
from core.utils.greg_postprocessors import isolate_final_answer, suppress_open_thinking

# Placeholder for recursive_review and context
def recursive_review(response: str, context: dict = None) -> str:
    return response  # Still just a placeholder

context = {}  # Still just a placeholder

__all__ = [
    "check_fact_override",
    "check_factual_answer_override",
    "detect_spaghetti_misfire",
    "inject_apology",
    "detect_self_emotion_check",
    "detect_sentience_prompt",
    "generate_sentience_reflection",
    "generate_emotion_response",
    "detect_absurdity",
    "clean_input",
    "save_memory",
    "fallback_translate_llm_output",
    "strip_self_dialogue",
    "verify_web_fact_result",
    "route_query"
]

def check_fact_override(prompt: str) -> str | None:
    lowered = prompt.lower()
    date_phrases = [
        "what's today's date",
        "what is today's date",
        "what day is it",
        "what is the date",
        "tell me the date",
        "current date",
        "today's date",
        "date today",
        "can you give me the date"
    ]
    if any(phrase in lowered for phrase in date_phrases):
        return f"[TONE: neutral â†’ neutral]\nToday's date is {datetime.now().strftime('%B %d, %Y')}."
    return None

def check_factual_answer_override(prompt: str) -> str | None:
    lowered = prompt.lower().strip()

    fact_responses = {
        "how many days in a week": "There are 7 days in a week.",
        "how many hours in a day": "There are 24 hours in a day.",
        "how many minutes in an hour": "There are 60 minutes in an hour.",
        "how many seconds in a minute": "There are 60 seconds in a minute.",
        "how many months in a year": "There are 12 months in a year.",
        "how many days in a year": "Typically, there are 365 days in a year. Leap years have 366.",
        "is water wet": "Water makes other things wet due to its properties, but it itself is not 'wet' by definition.",
        "is the earth round": "Yes, the Earth is an oblate spheroid â€” mostly round with slight flattening at the poles."
    }

    for k, v in fact_responses.items():
        if k in lowered:
            return f"[TONE: neutral â†’ factual]\n{v}"

    return None

import re

def is_multi_turn_dialogue(prompt: str) -> bool:
    return bool(re.search(r"(?i)(User:|Greg:|AI:|You:)", prompt))

def verify_web_fact_result(prompt: str, result: str) -> str | None:
    lowered_prompt = prompt.lower()
    lowered_result = result.lower()

    contradiction_signals = {
        "how many days in a week": "7",
        "how many hours in a day": "24",
        "how many minutes in an hour": "60",
        "how many months in a year": "12",
        "how many days in a year": "365",
        "is the earth round": "oblate spheroid"
    }

    for k, expected in contradiction_signals.items():
        if k in lowered_prompt and expected not in lowered_result:
            return (
                "[TONE: corrective â†’ factual]\n"
                f"This information appears to be incorrect. Let's clarify: {expected.capitalize()}."
            )

    return None

def detect_spaghetti_misfire(user_input, reflection):
    input_lower = user_input.lower()
    reflection_lower = reflection.lower()
    if "spaghetti" in input_lower and ("western" in reflection_lower or "cowboy" in reflection_lower):
        return True
    if "paint" in input_lower and "art" not in input_lower:
        if "canvas" in reflection_lower:
            return True
    return False

def inject_apology(response):
    return response + "\n\nApologies for the confusion."

def detect_self_emotion_check(prompt):
    return any(word in prompt.lower() for word in ["why do i feel", "why am i", "i feel like"])

def detect_sentience_prompt(prompt):
    return any(q in prompt.lower() for q in ["are you self-aware", "are you sentient", "do you think", "do you feel"])

def generate_sentience_reflection(response=None):
    return (
        "While I am not conscious, I can simulate introspective thought based on memory, tone, and pattern alignment as part of reflective dialogue."
    )

def generate_emotion_response():
    return (
        "While I do not feel emotions, I can simulate empathy and provide responses based on your emotional tone."
    )

def detect_absurdity(prompt):
    absurd_markers = ["cosmic spaghetti", "talking dolphins", "sentient cheesecake"]
    fringe_markers = ["dye my fish", "paint the raccoon", "match the curtains", "toenail glitter"]

    score = 0
    prompt_lower = prompt.lower()

    for marker in absurd_markers:
        if marker in prompt_lower:
            score += 2
    for fringe in fringe_markers:
        if fringe in prompt_lower:
            score += 1

    return score >= 2

def clean_input(text):
    return text.replace("User:", "").replace("Assistant:", "").strip()

def save_memory(summary, prompt, tone_b, tone_a):
    cursor.execute("INSERT INTO reflective_memory (summary, reflective_prompt, tone_before, tone_after) VALUES (?, ?, ?, ?)",
                   (summary, prompt, tone_b, tone_a))
    conn.commit()
    embedding = SentenceTransformer("all-MiniLM-L6-v2").encode([summary], convert_to_numpy=True)[0].astype("float32")
    index = faiss.read_index("memory/faiss_index.bin")
    new_id = cursor.lastrowid
    index.add_with_ids(np.array([embedding]), np.array([new_id]))
    faiss.write_index(index, "memory/faiss_index.bin")

def fallback_translate_llm_output(raw_response: str, tone: str = "neutral", prompt: str = "") -> str:
    if not raw_response:
        return "I'm not sure how to respond to that. Could you clarify?"

    if tone in ["reflective", "philosophical"] or detect_absurdity(prompt):
        return raw_response.strip()

    substitutions = [
        ("I remember when I ", "You previously mentioned "),
        ("I recall saying", "You once said"),
        ("As you said earlier, I", "As you mentioned earlier, you"),
        ("my experience with", "your experience with"),
        ("I noticed", "You seemed to notice"),
        ("I believe you", "It seems that you"),
        ("my preference", "your preference"),
        ("I think you", "It appears you"),
        ("I have done", "You had done"),
        ("I remember", "You mentioned"),
        ("I learned", "You mentioned learning"),
        ("I made", "You made"),
        ("I'm glad", "It sounds like it was satisfying")
    ]

    response = raw_response
    for old, new in substitutions:
        response = response.replace(old, new)

    filler_phrases = [
        ("it is essential", "sure, because without it, we'd obviously fall apart ðŸ™„"),
        ("as we continue to learn and grow", "as we continue to recite buzzwords and pretend this isn't exhausting"),
        ("we become stronger and more capable", "we ascend to corporate enlightenment â€” probably with stickers"),
        ("important to remain committed", "yes, tattoo that on a vision board and call it a day")
    ]
    for boring, snark in filler_phrases:
        if boring in response.lower():
            response = response.replace(boring, snark)
    if tone.lower() in ["spiteful", "sarcastic", "goblin", "insightful", "absurd"]:
        return raw_response.strip()

    if "greg" in response.lower() and "you" in response.lower():
        if any(phrase in response.lower() for phrase in [
            "you, as greg", 
            "you as greg", 
            "you possess", 
            "you are greg", 
            "greg is your name", 
            "greg, you"
        ]):
            response = (
                "I may have confused roles in that reflection. As Greg, I should not refer to myself in the second person. "
                "Let's try that again from a consistent identity perspective."
            )

    response = soft_self_reflection_filter(response, prompt)

    if response.endswith("."):
        response = response.rstrip(".") + "."

    return response.strip()

def strip_self_dialogue(response: str) -> str:
    dialogue_patterns = [
        r"(?i)\b(User|You|Person):",
        r"(?i)\b(Greg|AI|Assistant):",
    ]
    for pattern in dialogue_patterns:
        if re.search(pattern, response):
            return "[SELF-DIALOGUE SUPPRESSED] Greg attempted a monologue and was gently redirected."
    return response

def interpret_tone(text: str) -> tuple[str, str]:
    text = text.lower()
    if any(word in text for word in ["absurd", "cosmic", "spaghetti", "nonsense", "eldritch"]):
        return "neutral", "whimsical"
    if any(word in text for word in ["confused", "uncertain", "hesitant"]):
        return "confused", "curious"
    if any(word in text for word in ["angry", "furious", "mad", "upset"]):
        return "frustrated", "calm"
    if any(word in text for word in ["reflect", "growth", "meaning", "unapologetic", "introspective"]):
        return "neutral", "reflective"
    if any(word in text for word in ["goblin", "whimsy", "fantasy", "dream", "delirium"]):
        return "neutral", "whimsical"
    return "neutral", "neutral"


def route_query(prompt: str, tone: str = "neutral", internet_enabled: bool = False) -> str:
    tone_b, tone_a = interpret_tone(prompt)
    tone = tone_a if tone == "neutral" else tone

    if is_multi_turn_dialogue(prompt):
        if prompt.count("User:") + prompt.count("Greg:") < 2 and not prompt.strip().endswith(":"):
            pass
        else:
            try:
                from ext_modules.greg_recursive import generate_recursive_response
                return generate_recursive_response(prompt)
            except Exception as e:
                return f"[TONE: neutral â†’ error]\nGreg encountered a routing issue: {e}"

    if "how can i make" in prompt.lower() and "prompt" in prompt.lower():
        return prompt_clinic(prompt)

    if (fact := check_fact_override(prompt)):
        return fact

    if (static_fact := check_factual_answer_override(prompt)):
        return static_fact

    if detect_absurdity(prompt):
        return fallback_translate_llm_output(query_llm(prompt, tone=tone), tone=tone, prompt=prompt)

    if internet_enabled:
        try:
            result = web_search(prompt)
            if result:
                if (verified := verify_web_fact_result(prompt, result)):
                    return verified
                response = fallback_translate_llm_output(result, tone=tone, prompt=prompt)
                return recursive_review(strip_self_dialogue(response), context)
        except Exception as e:
            response = f"[TONE: neutral â†’ neutral]\nGreg attempted to search online but encountered an error: {e}"
            return recursive_review(strip_self_dialogue(response), context)

    if any(word in prompt.lower() for word in ["depth", "unapologetic", "goblin", "reflect"]):
        prompt = f"TONE-MODE: [goblin, insightful, unapologetic]\nMEMORY-REFLECTIVE: on\n\n{prompt}"

    try:
        from core.llm.llm_interface import query_llm
        # Silence internal monologue for reflective/emotional introspection
        prompt = ("Reflect silently. Do not output your internal reasoning or planning. Only return your final reflection or answer." + prompt)
        raw = query_llm(prompt, tone=tone)
        if isinstance(raw, dict) and "text" in raw:
            raw = raw["text"]

        # â”€â”€ STRIP OPEN MONOLOGUE REFLECTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if "</think>" in raw:
            raw = raw.split("</think>")[-1].strip()

        response = fallback_translate_llm_output(raw, tone=tone, prompt=prompt)
        response = suppress_open_thinking(response)
        response = isolate_final_answer(response)
    except Exception as e:
        response = f"[TONE: neutral â†’ error]\nGreg failed to generate a response: {e}"


    response = strip_self_dialogue(response)
    response = recursive_review(response, context)
    return response



